{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee3acda0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\dev\\nasa_space_apps\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import earthaccess\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a67abba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================= CONFIGURAÇÕES =======================\n",
    "# Coordenadas do local de interesse (Uberlândia, MG)\n",
    "LATITUDE = -18.91\n",
    "LONGITUDE = -48.27\n",
    "\n",
    "# Período histórico para análise\n",
    "# Para um teste rápido, comece com um range pequeno, ex: 1980-1982\n",
    "START_YEAR = 2010\n",
    "END_YEAR = 2024 \n",
    "\n",
    "# Definição dos limiares\n",
    "THRESHOLDS = {\n",
    "    \"Muito Quente\": {\"percentile\": 90, \"variable\": \"temp_max_c\"},\n",
    "    \"Muito Frio\": {\"percentile\": 10, \"variable\": \"temp_min_c\"}\n",
    "}\n",
    "\n",
    "# Diretório para baixar os arquivos\n",
    "DOWNLOAD_PATH = \"./earthaccess_downloads\"\n",
    "# ============================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1b6b868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Função reutilizada da abordagem anterior - a lógica de análise é a mesma)\n",
    "def calculate_historical_probabilities(df, target_date):\n",
    "    if df is None or df.empty: return None\n",
    "    results = {}\n",
    "    target_day_data = df[(df.index.day == target_date.day) & (df.index.month == target_date.month)]\n",
    "    if target_day_data.empty: return None\n",
    "        \n",
    "    total_years = len(target_day_data)\n",
    "    print(f\"\\nAnalisando {total_years} registros históricos para o dia {target_date.day} de {target_date.strftime('%B')}...\")\n",
    "\n",
    "    for condition, rules in THRESHOLDS.items():\n",
    "        variable = rules['variable']\n",
    "        threshold_value = df[variable].quantile(rules['percentile'] / 100.0)\n",
    "        \n",
    "        if rules['percentile'] > 50:\n",
    "            count_met = target_day_data[target_day_data[variable] > threshold_value].shape[0]\n",
    "            comparison = f\"> {threshold_value:.1f}°C\"\n",
    "        else:\n",
    "            count_met = target_day_data[target_day_data[variable] < threshold_value].shape[0]\n",
    "            comparison = f\"< {threshold_value:.1f}°C\"\n",
    "\n",
    "        probability = (count_met / total_years) * 100\n",
    "        results[condition] = {\"limiar\": comparison, \"probabilidade\": f\"{probability:.1f}%\"}\n",
    "        \n",
    "    return results\n",
    "\n",
    "def fetch_and_extract_with_earthaccess(year, lat, lon):\n",
    "    print(f\"\\n--- Processando ano: {year} ---\")\n",
    "    try:\n",
    "        results = earthaccess.search_data(\n",
    "            short_name=\"M2T1NXSLV\",\n",
    "            version='5.12.4',\n",
    "            temporal=(f'{year}-01-01', f'{year}-12-31'),\n",
    "            bounding_box=(lon - 0.5, lat - 0.5, lon + 0.5, lat + 0.5)\n",
    "        )\n",
    "        if not results:\n",
    "            print(f\"Nenhum dado encontrado para o ano {year}.\")\n",
    "            return None\n",
    "\n",
    "        print(f\"Baixando {len(results)} arquivos para o ano {year}...\")\n",
    "        local_path_year = os.path.join(DOWNLOAD_PATH, str(year))\n",
    "        downloaded_files = earthaccess.download(results, local_path=local_path_year)\n",
    "\n",
    "        print(\"Extraindo dados do ponto de interesse...\")\n",
    "        with xr.open_mfdataset(downloaded_files, combine='by_coords') as ds:\n",
    "            point_ds = ds[['T2MMAX', 'T2MMIN']].sel(lat=lat, lon=lon, method='nearest')\n",
    "            df_year = point_ds.to_dataframe()\n",
    "\n",
    "        shutil.rmtree(local_path_year)\n",
    "        print(f\"Arquivos para o ano {year} processados e limpos.\")\n",
    "        \n",
    "        return df_year\n",
    "    except Exception as e:\n",
    "        print(f\"Falha ao processar o ano {year}. Erro: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d6facb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autenticação com Earthdata bem-sucedida!\n",
      "\n",
      "--- Processando ano: 2010 ---\n",
      "Baixando 365 arquivos para o ano 2010...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "QUEUEING TASKS | : 100%|██████████| 365/365 [00:00<00:00, 32631.80it/s]\n",
      "PROCESSING TASKS | :  53%|█████▎    | 192/365 [20:28<12:48,  4.44s/it]"
     ]
    }
   ],
   "source": [
    "# Autenticação (Lembrete: certifique-se que seu arquivo .netrc está configurado!)\n",
    "try:\n",
    "    auth = earthaccess.login()\n",
    "    if not auth.authenticated:\n",
    "        raise Exception(\"Falha na autenticação. Verifique seu arquivo .netrc ou credenciais.\")\n",
    "    print(\"Autenticação com Earthdata bem-sucedida!\")\n",
    "except Exception as e:\n",
    "    print(f\"Erro durante a autenticação: {e}\")\n",
    "\n",
    "# Loop principal para buscar e processar os dados\n",
    "all_years_dfs = []\n",
    "if auth.authenticated:\n",
    "    for current_year in range(START_YEAR, END_YEAR + 1):\n",
    "        df_for_year = fetch_and_extract_with_earthaccess(current_year, LATITUDE, LONGITUDE)\n",
    "        if df_for_year is not None:\n",
    "            all_years_dfs.append(df_for_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94595f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Arquivo com dados horários salvo em: dados_horarios.csv\n"
     ]
    }
   ],
   "source": [
    "import netrc\n",
    "import requests\n",
    "from requests.auth import HTTPBasicAuth\n",
    "import pandas as pd\n",
    "import io\n",
    "\n",
    "# 1. Parâmetros do usuário\n",
    "lat = -18.91\n",
    "lon = -48.27\n",
    "time_start = \"2000-01-01T00:00:00\"\n",
    "time_end = \"2020-09-30T21:00:00\"\n",
    "data = \"GLDAS_NOAH025_3H_2_1_Tair_f_inst\"  # variável NASA/GLDAS: temperatura do ar instantânea\n",
    "\n",
    "# 2. Autenticação Earthdata (leitura do .netrc)\n",
    "# Certifique-se de ter ~/.netrc com:\n",
    "# machine urs.earthdata.nasa.gov login SEU_LOGIN password SUA_SENHA\n",
    "signin_url = \"https://api.giovanni.earthdata.nasa.gov/signin\"\n",
    "creds = netrc.netrc().authenticators('urs.earthdata.nasa.gov')\n",
    "if creds is None:\n",
    "    raise RuntimeError(\"Credenciais Earthdata não encontradas em ~/.netrc!\")\n",
    "user, _, pwd = creds\n",
    "token = requests.get(signin_url, auth=HTTPBasicAuth(user, pwd), allow_redirects=True).text.replace('\"','')\n",
    "\n",
    "# 3. Função para obter a série temporal\n",
    "def call_time_series(lat, lon, time_start, time_end, data, token):\n",
    "    query_parameters = {\n",
    "        \"data\": data,\n",
    "        \"location\": f\"[{lat},{lon}]\",\n",
    "        \"time\": f\"{time_start}/{time_end}\"\n",
    "    }\n",
    "    headers = {\"authorizationtoken\": token}\n",
    "    response = requests.get(\"https://api.giovanni.earthdata.nasa.gov/timeseries\",\n",
    "                            params=query_parameters, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        raise RuntimeError(f\"Erro na requisição: HTTP {response.status_code} - {response.text[:300]}\")\n",
    "    return response.text\n",
    "\n",
    "# 4. Chamar API e carregar CSV em pandas\n",
    "csv_text = call_time_series(lat, lon, time_start, time_end, data, token)\n",
    "\n",
    "lines = csv_text.splitlines()\n",
    "header_index = next(i for i, line in enumerate(lines) if line.strip().lower().startswith(\"timestamp (utc),\"))\n",
    "table_csv = \"\\n\".join(lines[header_index:])\n",
    "df = pd.read_csv(io.StringIO(table_csv))\n",
    "\n",
    " # Converte para datetime e Celsius\n",
    "df['time'] = pd.to_datetime(df.iloc[:, 0])\n",
    "df.set_index('time', inplace=True)\n",
    "df['temp_c'] = df['Data'] - 273.15\n",
    "\n",
    "# 1. Salvar os dados HORÁRIOS\n",
    "hourly_df = df[['temp_c']].dropna()\n",
    "output_hourly = 'dados_horarios_temperatura.csv'\n",
    "hourly_df.to_csv(output_hourly)\n",
    "print(f\"-> Arquivo com dados horários salvo em: {output_hourly}\")\n",
    "\n",
    "# 2. Agrupar por dia e salvar os dados DIÁRIOS\n",
    "daily_df = df.resample('D').agg(\n",
    "    temp_avg_c=('temp_c', 'mean'),\n",
    "    temp_max_c=('temp_c', 'max'),\n",
    "    temp_min_c=('temp_c', 'min')\n",
    ")\n",
    "\n",
    "daily_df = df.resample('D').agg(\n",
    "    temp_avg_c=('temp_c', 'mean'),\n",
    "    temp_max_c=('temp_c', 'max'),\n",
    "    temp_min_c=('temp_c', 'min')\n",
    ")\n",
    "    \n",
    "final_df = daily_df.dropna()\n",
    "\n",
    "# Salva o arquivo final que a API Flask vai usar\n",
    "output_filename = 'dados_historicos_tempeartura.csv'\n",
    "final_df.to_csv(output_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa97eb54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nenhum dado foi processado com sucesso. Verifique a saída da célula anterior para erros.\n"
     ]
    }
   ],
   "source": [
    "if not all_years_dfs:\n",
    "    print(\"Nenhum dado foi processado com sucesso. Verifique a saída da célula anterior para erros.\")\n",
    "else:\n",
    "    # Combina os dataframes de todos os anos em um só\n",
    "    print(\"\\nCombinando todos os anos em um único dataset...\")\n",
    "    historical_df = pd.concat(all_years_dfs)\n",
    "\n",
    "    # Limpeza final e conversão de Kelvin para Celsius\n",
    "    historical_df['temp_max_c'] = historical_df['T2MMAX'] - 273.15\n",
    "    historical_df['temp_min_c'] = historical_df['T2MMIN'] - 273.15\n",
    "    historical_df = historical_df[['temp_max_c', 'temp_min_c']]\n",
    "    \n",
    "    # Calcular as probabilidades\n",
    "    data_alvo = datetime.now()\n",
    "    probabilities = calculate_historical_probabilities(historical_df, data_alvo)\n",
    "\n",
    "    # Exibir os resultados\n",
    "    if probabilities:\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(f\"Análise de Probabilidade (via earthaccess) para Uberlândia, MG\")\n",
    "        print(f\"Data da Análise: {data_alvo.day} de {data_alvo.strftime('%B')}\") # Note que estamos em Outubro\n",
    "        print(f\"Baseado em dados de {START_YEAR} a {END_YEAR}\")\n",
    "        print(\"=\"*50)\n",
    "        for condition, result in probabilities.items():\n",
    "            print(f\"Condição:       {condition}\")\n",
    "            print(f\"  ↳ Limiar:       Temperatura {result['limiar']}\")\n",
    "            print(f\"  ↳ Probabilidade: {result['probabilidade']}\")\n",
    "        print(\"=\"*50)\n",
    "\n",
    "# Dica: Inspecione o dataframe final a qualquer momento em uma nova célula\n",
    "# Basta digitar e executar:\n",
    "# historical_df.head()\n",
    "# historical_df.describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
